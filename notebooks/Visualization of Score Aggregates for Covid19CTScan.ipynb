{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "aggregator_file = \"./aggregate_scores_512.json\"\n",
    "scores = json.load(open(aggregator_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed702808",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "def is_same_experiment(e1, e2):\n",
    "    return e1['is_normalized'] == e2['is_normalized'] and e1['loss_type'] == e2['loss_type']\n",
    "\n",
    "def experiment_exists(exp):\n",
    "    exists = False\n",
    "\n",
    "    for r in results:\n",
    "        if is_same_experiment(exp, r):\n",
    "            exists = True\n",
    "\n",
    "    return exists\n",
    "\n",
    "# Each outer loop = each k\n",
    "for k in range(len(scores)):\n",
    "    for experiment in scores[k]:\n",
    "        if not experiment_exists(experiment['experiment']):\n",
    "            result_obj = copy.deepcopy(experiment['experiment'])\n",
    "            result_obj['ave_macro_score'] = 0\n",
    "\n",
    "            for i in range(num_classes):\n",
    "                result_obj['class_{}'.format(i)] = 0\n",
    "\n",
    "            results.append(result_obj)\n",
    "\n",
    "for result_obj_index in range(len(results)):\n",
    "    for k in range(len(scores)):\n",
    "        for exp_obj in scores[k]:\n",
    "            result_obj = results[result_obj_index]\n",
    "            if is_same_experiment(exp_obj['experiment'], result_obj):\n",
    "                results[result_obj_index]['ave_macro_score'] += exp_obj['ave_macro_score']\n",
    "\n",
    "                for label_index in range(num_classes):\n",
    "                    ave_score = 0\n",
    "                    for label_score_set in  exp_obj['label_scores']:\n",
    "                        if label_index > len(label_score_set) - 1:\n",
    "                            ave_score += 1\n",
    "                        else:\n",
    "                            ave_score += label_score_set[label_index]\n",
    "\n",
    "                    ave_score /= len(exp_obj['label_scores'])\n",
    "                    results[result_obj_index]['class_{}'.format(label_index)] += ave_score\n",
    "\n",
    "# Average out\n",
    "for result_obj in results:\n",
    "    result_obj['ave_macro_score'] = result_obj['ave_macro_score'] / len(scores)\n",
    "\n",
    "    for label_index in range(num_classes):\n",
    "        result_obj['class_{}'.format(label_index)] /= len(scores)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c31caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_types = []\n",
    "\n",
    "for result in results:\n",
    "    model_types.append(\"{}-{}\".format(result['loss_type'], result['is_normalized']))\n",
    "\n",
    "df = pd.DataFrame(results, index=model_types)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162eed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = []\n",
    "\n",
    "metrics = [\n",
    "    'ave_macro_score',\n",
    "    'class_0',\n",
    "    'class_1',\n",
    "    'class_2',\n",
    "    'class_3'\n",
    "]\n",
    "\n",
    "for metric in metrics:\n",
    "    best_score = 0.001\n",
    "    best_model = ''\n",
    "    improvement_raw = 0.0\n",
    "    for result in results:\n",
    "        if result[metric] > best_score:\n",
    "            improvement_raw = result[metric] - best_score\n",
    "            best_score = result[metric]\n",
    "            best_model = \"{}-{}\".format(result['loss_type'], result['is_normalized'])\n",
    "\n",
    "    best_models.append({'metric': metric, 'best_model': best_model, 'improvement_raw': improvement_raw })\n",
    "\n",
    "df_best_models = pd.DataFrame(best_models)\n",
    "\n",
    "df_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409755c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
